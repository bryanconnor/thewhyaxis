Great questions! Here are our responses:

# Where did the impetus come from to start this project? What was the goal? 
The project was part of a data visualization class that we both took at the Worcester Polytechnic Institute (WPI).  We could pick any dataset and went through several different datasets before deciding on the MBTA data.  It appealed to us for couple of reasons, first we both ride the T but in different contexts.  Mike commutes and rides the trains every weekday, Brian takes them into the city on weekends or weekday nights.  We also wanted to build a project that would be interesting to a wide range of people.  We were familiar with several historic train visualizations which only had schedule data to visualize.  There also seemed to be several apps built of off the MBTA’s real time data but fewer reports that did historical analysis.  We thought that armed with this new real-time data and building on ideas from historic works that there was a chance to provide this data in a form that people didn’t already have.

# Were there any important constraints? Technical? Timing? Data?
The development of the project was paced around the structure of the class, which focused the majority of the time on understanding what type of questions to answer and designing the visualization.  Our project was ambitious so it was tough to finish the final implementation in time.  One question that we really wanted to answer was ‘how crowded is this train,’ which is why we reached out to the MBTA for the turnstile information.  It turns out that we couldn’t truthfully answer that question because the turnstiles don’t always accurately count the number of people exiting the stations, so we came up with other visualizations that presented this data but without trying to address that question.  Getting the turnstile data did add a significant amount of depth to the project though and we were very excited with the MBTA provided it.  Another constraint was that we wanted to make this available online but we didn’t want to spend any money.  All of the tools and techniques that we used to collect the data, process it, create the visualizations and host them online were freely available.

# What was the process like to gather feedback from the data viz community on twitter? 
We received feedback on the visualization from many sources.  During the class we gave four different presentations on the project and we received valuable feedback from the other students and professor which helped guide the design.  After posting our visualization on Reddit and Hacker News Brian engaged in conversations and got useful feedback in their discussion forums.  Mike is partial to Twitter since he works there, but we do think that Twitter was one of our best sources of feedback. Since the platform is public, real-time, conversational, and widely-adopted by the data visualization community, people from all over the world have been tagging Mike to provide feedback and ask questions.  To encourage this, we linked to Mike’s Twitter profile from the visualization and also added Twitter Card metadata that renders a links to his profile any time anyone tweets the link.  Since Mike didn’t have many Twitter followers, getting retweeted by Mike Bostock, Bret Victor, and Edward Tufte were invaluable in reaching out the larger community on Twitter.  We also presented the project at a data visualization meetup and got some great feedback from local visualization experts.  Overall the feedback received in person and online has been incredibly helpful.

# How do you hope people will use this? 
One goal was to try and provide some insight on simple questions that riders have about the system.  Riders know that the system gets more congested during rush hour from empirical observations but we thought that seeing data that confirms the congestion and sheds more light on its intricacies would be interesting to them.  We also wanted to try and make the data accessible and provide visualizations that people could explore.  One key part of the design was to show readers the interesting parts of the data so they can quickly get a high level understanding and we think this makes the visualizations much more accessible since readers can become oriented more quickly.

One goal that we did not have was to assist domain experts or urban planners in trying to understand how to improve the train system.  We actually observed from our analysis that the T runs smoothly through snow storms and localized traffic spikes and that pre-emptive scheduling decisions made by the MBTA’s own experts keep travel times consistent during normal daily congestion.

# Do you have any future plans for the visualization or for the real-time version? 
We’ve had a really great response from the online community and have received many requests for the source code and are looking at open sourcing the project now.  We made a simple real time app based on some of the designs from the visualization and are trying to gauge interest if people want to see more.  If you or your readers are interested in something like this let us know!

# Is there any other detail you want to fill in about the design process or other details?
As we mentioned in the handout Bret Victor’s essay Up and Down the Ladder of Abstraction was a big influence, Bret’s essays are also great examples of how to do interaction right.  We started out with very little interaction and color in the visualizations and only added those elements as needed.  Limiting the interaction to start also helped with mobile or users with old browsers.  Some print visualization techniques didn’t work out as well as we hoped, in particular small multiples didn’t work well for us in some cases because we couldn’t get the resolution we wanted out of the small graphics. As an alternative we used interaction to change the content of a single larger graphic.  We also did research reports on storytelling (http://vis.stanford.edu/files/2010-Narrative-InfoVis.pdf) and color in visualizations which influenced the design.  One thing we tried to focus on was not just to show interesting parts of the data, but also to explain what caused the data to behave that way.  We also had several more complicated designs but moved towards multiple simpler visualizations that each answer specific questions more clearly. 